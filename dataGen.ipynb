{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "data_path = os.getenv('DATA_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_videos_using_channel_id(youtube,channelId,maxResults = 10) :\n",
    "    videoIdList = []\n",
    "    request = youtube.search().list(\n",
    "        channelId=channelId,\n",
    "        maxResults=maxResults,\n",
    "        order=\"viewCount\",\n",
    "        part=\"snippet\"\n",
    "    )\n",
    "\n",
    "    response = request.execute()\n",
    "    if response and response['items']:\n",
    "        for item in response['items']:\n",
    "            if item['id']['kind'] == 'youtube#video':\n",
    "                videoIdList.append(item['id']['videoId'])\n",
    "\n",
    "    return videoIdList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replies_of_comments(youtube, commentId):\n",
    "    \n",
    "    nextPageToken = 'null'\n",
    "    replies = []    \n",
    "\n",
    "    while nextPageToken != '':\n",
    "        request = youtube.comments().list(\n",
    "            part= 'snippet',\n",
    "            parentId= commentId,\n",
    "            maxResults = 100,\n",
    "        )\n",
    "        if(nextPageToken != 'null'):\n",
    "            request = youtube.comments().list(\n",
    "            part= 'snippet',\n",
    "            parentId= commentId,\n",
    "            maxResults = 100,\n",
    "            pageToken= nextPageToken\n",
    "        )\n",
    "        \n",
    "        response = request.execute()\n",
    "        # return response\n",
    "        \n",
    "\n",
    "        for item in response['items']:\n",
    "            replies.append(dict(\n",
    "                id = item['id'],\n",
    "                textOriginal = item['snippet']['textOriginal'],\n",
    "                textDisplay = item['snippet']['textDisplay'],\n",
    "                authorDisplayName = item['snippet']['authorDisplayName'],\n",
    "                authorId = item['snippet']['authorChannelId']['value'],\n",
    "                parentId = item['snippet']['parentId'],\n",
    "                publishedAt = item['snippet']['publishedAt'],\n",
    "                updatedAt = item['snippet']['updatedAt'],\n",
    "                likeCount = item['snippet']['likeCount'],\n",
    "                \n",
    "            ))\n",
    "        if 'nextPageToken' in response:\n",
    "            nextPageToken = response['nextPageToken']\n",
    "        else:\n",
    "            nextPageToken = ''\n",
    "    \n",
    "    return replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_comments_optimized(youtube, video_id):\n",
    "    \n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet,replies\",\n",
    "        videoId=video_id,\n",
    "        order=\"relevance\",\n",
    "        maxResults=100\n",
    "    )\n",
    "    try:\n",
    "        response = request.execute() \n",
    "        # return response\n",
    "\n",
    "        comments = []\n",
    "        \n",
    "        for item in response['items']:\n",
    "            replies = []\n",
    "            if 'replies' in item:\n",
    "                if item['snippet']['totalReplyCount'] <= len(item['replies']['comments']):\n",
    "                    for reply in item['replies']['comments']:\n",
    "                        replies.append(dict(\n",
    "                            id = reply['id'],\n",
    "                            textOriginal = reply['snippet']['textOriginal'],\n",
    "                            textDisplay = reply['snippet']['textDisplay'],\n",
    "                            authorDisplayName = reply['snippet']['authorDisplayName'],\n",
    "                            authorId = reply['snippet']['authorChannelId']['value'],\n",
    "                            parentId = reply['snippet']['parentId'],\n",
    "                            publishedAt = reply['snippet']['publishedAt'],\n",
    "                            updatedAt = reply['snippet']['updatedAt'],\n",
    "                            likeCount = reply['snippet']['likeCount'],\n",
    "                        ))\n",
    "                else:\n",
    "                    replies = get_replies_of_comments(youtube, item['id'])\n",
    "\n",
    "            data = dict(\n",
    "                id = item['id'],\n",
    "                textOriginal = item['snippet']['topLevelComment']['snippet']['textOriginal'],\n",
    "                textDisplay = item['snippet']['topLevelComment']['snippet']['textDisplay'],\n",
    "                authorDisplayName = item['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
    "                authorId = item['snippet']['topLevelComment']['snippet']['authorChannelId']['value'],\n",
    "                replyCount = item['snippet']['totalReplyCount'],\n",
    "                publishedTime = item['snippet']['topLevelComment']['snippet']['publishedAt'],\n",
    "                updateTime = item['snippet']['topLevelComment']['snippet']['updatedAt'],\n",
    "                likeCount = item['snippet']['topLevelComment']['snippet']['likeCount'],\n",
    "                replies = replies\n",
    "            )\n",
    "            \n",
    "            \n",
    "            comments.append(data)\n",
    "        \n",
    "        return comments\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments_from_channels(youtube, channel_list, videos_per_channel=10):\n",
    "\n",
    "    os.makedirs(f'{data_path}/json', exist_ok=True)\n",
    "\n",
    "    for channel in channels:\n",
    "        videoIds = get_top_videos_using_channel_id(youtube, channel[\"channelId\"], videos_per_channel)\n",
    "        print(videoIds)\n",
    "\n",
    "        with open(f'{data_path}/json/{channel[\"name\"]}.json', \"a\") as f:\n",
    "            f.write('[')\n",
    "\n",
    "\n",
    "        for videoId in videoIds:\n",
    "            comments = get_video_comments_optimized(youtube, videoId)\n",
    "            videoData = dict(\n",
    "                channelId=channel[\"channelId\"],\n",
    "                channelName=channel[\"name\"],\n",
    "                videoId=videoId,\n",
    "                comments=comments,\n",
    "            )\n",
    "            \n",
    "            # appending the comments to the json file\n",
    "            with open(f'{data_path}/json/{channel[\"name\"]}.json', \"a\") as f:\n",
    "                json.dump(videoData, fp=f, indent=2)\n",
    "                f.write(\",\")\n",
    "            \n",
    "        \n",
    "        with open(f'{data_path}/json/{channel[\"name\"]}.json', 'rb+') as f:\n",
    "            f.seek(-1, 2)\n",
    "            f.truncate()\n",
    "\n",
    "        with open(f'{data_path}/json/{channel[\"name\"]}.json', \"a\") as f:\n",
    "            f.write(']')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [\n",
    "    dict(channelId = \"UCsBjURrPoezykLs9EqgamOA\",name = \"fireship\"),\n",
    "    # dict(channelId = \"UC8CX0LD98EDXl4UYX1MDCXg\",name = \"Valorant\"),\n",
    "    # dict(channelId = \"UCXIJgqnII2ZOINSWNOGFThA\",name = \"FoxNews\"),\n",
    "    # dict(channelId = \"UCUsN5ZwHx2kILm84-jPDeXw\",name = \"ComedyCentral\"),\n",
    "]\n",
    "\n",
    "channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_comments_from_channels(youtube, channel_list = channels, videos_per_channel = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_excel(channel_list):\n",
    "\n",
    "    os.makedirs(f'{data_path}/xlsx', exist_ok=True)\n",
    "\n",
    "    for channel in channel_list:\n",
    "        print(f'Extracting comment threads for {channel[\"name\"]}')\n",
    "        with open(f'{data_path}/json/{channel[\"name\"]}.json') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        comments = []\n",
    "\n",
    "        for item in data:\n",
    "            for comment in item['comments'] :\n",
    "                if comment['replyCount'] < 2:\n",
    "                    continue\n",
    "                cnt = 1\n",
    "                data = {\n",
    "                    's.no.' : cnt,\n",
    "                    'id' : comment['id'],\n",
    "                    'isParent' : True,\n",
    "                    'authorName' : comment['authorDisplayName'],\n",
    "                    'text' : comment['textOriginal'],\n",
    "                    'likeCount' : comment['likeCount'],\n",
    "                    'label(CyberBullying,Normal)' : ''\n",
    "                }\n",
    "                cnt+=1\n",
    "                comments.append(data)\n",
    "\n",
    "                for reply in comment['replies'][::-1]:\n",
    "                    reply = {\n",
    "                        's.no.' : cnt,\n",
    "                        'id' : reply['id'],\n",
    "                        'isParent' : False,\n",
    "                        'authorName' : reply['authorDisplayName'],\n",
    "                        'text' : reply['textOriginal'],\n",
    "                        'likeCount' : reply['likeCount'],\n",
    "                        'label(CyberBullying,Normal)' : ''\n",
    "                    }\n",
    "                    cnt+=1\n",
    "                    comments.append(reply)\n",
    "        \n",
    "        df = pd.DataFrame(comments)\n",
    "        \n",
    "        df.to_excel(f'{data_path}/xlsx/{channel[\"name\"]}_threads.xlsx')\n",
    "        print(f'Extracted comment threads for {channel[\"name\"]}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_to_excel(channel_list = channels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
