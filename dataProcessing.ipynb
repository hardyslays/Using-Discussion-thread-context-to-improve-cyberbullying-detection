{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhima\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import emoji\n",
    "import torch\n",
    "from torch.nn import ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "data_path = os.getenv('DATA_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "labels = ['negative', 'neutral', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(arr):\n",
    "    processed = []\n",
    "\n",
    "    for i in range(len(arr)):\n",
    "        new_text = []\n",
    "        \n",
    "        c = 1\n",
    "        for t in arr[i].split(\" \"):\n",
    "            if c > 150:\n",
    "                break\n",
    "            t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "            t = 'http' if t.startswith('http') else t\n",
    "            new_text.append(t)\n",
    "            c+=1\n",
    "        \n",
    "        processed.append(\" \".join(new_text))\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractNegativeSentimentValues(arr):\n",
    "    processed = preprocess(arr)\n",
    "    out = []\n",
    "    \n",
    "    for text in processed:\n",
    "        # tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "        outputs = model(**inputs)\n",
    "        scores = outputs[0][0].detach().numpy()\n",
    "        probs = softmax(scores)\n",
    "        # out.append(probs)\n",
    "        print(labels[0],probs[0])\n",
    "        out.append(probs[0])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = pd.read_csv('./bad-words.csv')\n",
    "bad_words = bad_words.to_numpy()\n",
    "bad_words\n",
    "\n",
    "def countBadWords(data):\n",
    "    bad_words_count = []\n",
    "    for i, row in data.iterrows():\n",
    "        text = str(row['text'])\n",
    "        # print(text)\n",
    "        bad_words_count.append(sum(text.count(str(word)) for word in bad_words))\n",
    "    data['bad_words_count'] = bad_words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindAuthorId(authorToId, comment, default):\n",
    "            if comment[0] != '@':\n",
    "                return default\n",
    "            else:\n",
    "                reg = comment.split(' ', 1)[0][1:]\n",
    "                matches = [author for author in authorToId if author.split(' ', 1)[0] == reg]\n",
    "\n",
    "                for el in matches:\n",
    "                    t = '@' + el\n",
    "                    if comment.find(t) == 0:\n",
    "                        return authorToId[el]\n",
    "                return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "elu = ELU(alpha=0.2)\n",
    "\n",
    "def adjustedScoreCalc(curScore, parentScore):\n",
    "    c = torch.tensor(curScore)\n",
    "    p = torch.tensor(parentScore)\n",
    "    return torch.Tensor.item(c + 0.7*elu(p-c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processChannelData(channel_list):\n",
    "    \n",
    "    os.makedirs(f'{data_path}/processed', exist_ok=True)\n",
    "\n",
    "    for channel in channel_list: \n",
    "\n",
    "        print(\"Processing channel: \", channel['name'])\n",
    "        \n",
    "        # Read channel data\n",
    "        data = pd.read_excel(f'{data_path}/xlsx/{channel[\"name\"]}_threads.xlsx')\n",
    "        \n",
    "        #Extract negative sentiments of individual comments\n",
    "        probs = extractNegativeSentimentValues(list(data['text'].values.astype(str)))\n",
    "        data['negative_prob'] = probs\n",
    "\n",
    "        #Rename and drop some labels\n",
    "        data = data.rename(columns={f\"label(CyberBullying,Normal)\":\"label\"})\n",
    "        data.drop(columns=['Unnamed: 0','Unnamed: 0.1','id'], inplace=True, errors='ignore')\n",
    "\n",
    "        #Counting bad words\n",
    "        countBadWords(data)\n",
    "\n",
    "        # Getting parent comments\n",
    "        parentIdx = list(data[data['s.no.'] == 1].index)\n",
    "        parentIdx.append(len(data))\n",
    "\n",
    "        # Creating seperate threads\n",
    "        threads = []\n",
    "        for i in range(len(parentIdx)-1):\n",
    "            if i == len(parentIdx)-1:\n",
    "                threads.append(data.iloc[parentIdx[i]:].copy())\n",
    "            else:\n",
    "                threads.append(data.iloc[parentIdx[i]:parentIdx[i+1]].copy())\n",
    "\n",
    "\n",
    "        # Labelling authors and creating tree structure\n",
    "        le = LabelEncoder()\n",
    "        data = data.assign(repliedTo = '0')\n",
    "\n",
    "        for i in range(len(threads)):\n",
    "            cur_thread = threads[i].copy()\n",
    "            authors = [str(author) for author in cur_thread['authorName'].values]\n",
    "            \n",
    "            cur_thread['authorName'] = le.fit_transform(authors)\n",
    "            authorToId = dict(zip(authors, cur_thread['authorName']))\n",
    "\n",
    "            replied_to = []\n",
    "            default = cur_thread['authorName'].values[0]\n",
    "            for j, row in cur_thread.iterrows():\n",
    "                text = str(row['text'])\n",
    "                if text[0] != '@':\n",
    "                    replied_to.append(default)\n",
    "                else:\n",
    "                    replied_to.append(FindAuthorId(authorToId, text, default))\n",
    "            cur_thread['repliedTo'] = replied_to\n",
    "            threads[i] = cur_thread\n",
    "            \n",
    "            # Calculating adjusted sentiment scores of comments\n",
    "\n",
    "        for i in range(len(threads)):\n",
    "            cur_thread = threads[i]\n",
    "            cur_thread['adjusted_sentiment'] = cur_thread['negative_prob'].values\n",
    "\n",
    "            for j, row in cur_thread.iterrows():\n",
    "                if j == 0:\n",
    "                    continue\n",
    "                cur_thread['adjusted_sentiment'][j] = adjustedScoreCalc(row['negative_prob'], cur_thread[cur_thread['authorName'] == row['repliedTo']]['negative_prob'].values[0])\n",
    "\n",
    "        processed_df = pd.concat(threads)\n",
    "        processed_df.to_csv(f'{data_path}/processed/{channel[\"name\"]}_processed_data.csv', index=False)\n",
    "            \n",
    "        print(\"Processed channel: \", channel['name'])\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'channelId': 'UCsBjURrPoezykLs9EqgamOA', 'name': 'fireship'},\n",
       " {'channelId': 'UC8CX0LD98EDXl4UYX1MDCXg', 'name': 'Valorant'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_list = [\n",
    "    dict(channelId = \"UCsBjURrPoezykLs9EqgamOA\",name = \"fireship\"),\n",
    "    dict(channelId = \"UC8CX0LD98EDXl4UYX1MDCXg\",name = \"Valorant\"),\n",
    "    # dict(channelId = \"UCXIJgqnII2ZOINSWNOGFThA\",name = \"FoxNews\"),\n",
    "    # dict(channelId = \"UCUsN5ZwHx2kILm84-jPDeXw\",name = \"ComedyCentral\"),\n",
    "]\n",
    "\n",
    "channel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing channel:  fireship\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Dataset/xlsx/fireship_threads.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m processChannelData(channel_list \u001b[39m=\u001b[39;49m channel_list)\n",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m, in \u001b[0;36mprocessChannelData\u001b[1;34m(channel_list)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mProcessing channel: \u001b[39m\u001b[39m\"\u001b[39m, channel[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      9\u001b[0m \u001b[39m# Read channel data\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_excel(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mdata_path\u001b[39m}\u001b[39;49;00m\u001b[39m/xlsx/\u001b[39;49m\u001b[39m{\u001b[39;49;00mchannel[\u001b[39m\"\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m}\u001b[39;49;00m\u001b[39m_threads.xlsx\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     12\u001b[0m \u001b[39m#Extract negative sentiments of individual comments\u001b[39;00m\n\u001b[0;32m     13\u001b[0m probs \u001b[39m=\u001b[39m extractNegativeSentimentValues(\u001b[39mlist\u001b[39m(data[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\excel\\_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    481\u001b[0m     should_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     io \u001b[39m=\u001b[39m ExcelFile(io, storage_options\u001b[39m=\u001b[39;49mstorage_options, engine\u001b[39m=\u001b[39;49mengine)\n\u001b[0;32m    483\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39mand\u001b[39;00m engine \u001b[39m!=\u001b[39m io\u001b[39m.\u001b[39mengine:\n\u001b[0;32m    484\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\excel\\_base.py:1652\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     ext \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxls\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1651\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1652\u001b[0m     ext \u001b[39m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1653\u001b[0m         content_or_path\u001b[39m=\u001b[39;49mpath_or_buffer, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m   1654\u001b[0m     )\n\u001b[0;32m   1655\u001b[0m     \u001b[39mif\u001b[39;00m ext \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1656\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1657\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExcel file format cannot be determined, you must specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1658\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39man engine manually.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1659\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\excel\\_base.py:1525\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(content_or_path, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1523\u001b[0m     content_or_path \u001b[39m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1525\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1526\u001b[0m     content_or_path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1527\u001b[0m ) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m   1528\u001b[0m     stream \u001b[39m=\u001b[39m handle\u001b[39m.\u001b[39mhandle\n\u001b[0;32m   1529\u001b[0m     stream\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    866\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Dataset/xlsx/fireship_threads.xlsx'"
     ]
    }
   ],
   "source": [
    "processChannelData(channel_list = channel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{data_path}/processed/{channel_list[0][\"name\"]}_processed_data.csv')\n",
    "\n",
    "for i in range(1, len(channel_list)):\n",
    "    df = df.append(pd.read_csv(f'{data_path}/processed/{channel_list[i][\"name\"]}_processed_data.csv'))\n",
    "\n",
    "df.to_csv(f'{data_path}/processed/Final_processed_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
